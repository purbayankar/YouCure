# -*- coding: utf-8 -*-
"""SymptomSuggestion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TCme3BRC34OqIgLUca6GkivYK-1HFs-j
"""

# !git clone https://github.com/rahul15197/Disease-Detection-based-on-Symptoms

# cd Disease-Detection-based-on-Symptoms

"""# **Disease Detection using Symptoms and Treatment recommendation**

This notebook contains code to detect disease using the symptoms entered and selected by the user and recommends the appropriate treatments.

"""

# Predicts diseases based on the symptoms entered and selected by the user.
# importing all necessary libraries
import warnings
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.model_selection import train_test_split, cross_val_score
from statistics import mean
from nltk.corpus import wordnet 
import requests
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import RegexpTokenizer
from itertools import combinations
from time import time
from collections import Counter
import operator
# from xgboost import XGBClassifier
import math
# from Treatment import diseaseDetail
from sklearn.linear_model import LogisticRegression
import pickle
from sklearn.pipeline import make_pipeline
warnings.simplefilter("ignore")

"""Download resources required for NLTK pre-processing"""

import nltk
nltk.download('stopwords') 
nltk.download('wordnet')

"""**synonyms function** finds the synonymous terms of a symptom entered by the user.

This is necessary as the user may use a term for a symptom which may be different from the one present in dataset.
This improves the accuracy by reducing the wrong predictions even when symptoms for a disease are entered slightly different than the ones on which model is trained.

*Synonyms are searched on Thesaurus.com and NLTK Wordnet*
"""

# returns the list of synonyms of the input word from thesaurus.com (https://www.thesaurus.com/) and wordnet (https://www.nltk.org/howto/wordnet.html)
import re
from googlesearch import search
import warnings
warnings.filterwarnings("ignore")
import requests
from bs4 import BeautifulSoup

# Take input a disease and return the content of wikipedia's infobox for that specific disease

def diseaseDetail(term):
    diseases=[term]
    ret=term+"\n"
    for dis in diseases:
        # search "disease wilipedia" on google 
        query = dis+' wikipedia'
        for sr in search(query+".co.in"): 
            # open wikipedia link
            match=re.search(r'wikipedia',sr)
            filled = 0
            if match:
                wiki = requests.get(sr,verify=False)
                soup = BeautifulSoup(wiki.content, 'html5lib')
                # Fetch HTML code for 'infobox'
                info_table = soup.find("table", {"class":"infobox"})
                if info_table is not None:
                    # Preprocess contents of infobox
                    for row in info_table.find_all("tr"):
                        data=row.find("th",{"scope":"row"})
                        if data is not None:
                            symptom=str(row.find("td"))
                            symptom = symptom.replace('.','')
                            symptom = symptom.replace(';',',')
                            symptom = symptom.replace('<b>','<b> \n')
                            symptom=re.sub(r'<a.*?>','',symptom) # Remove hyperlink
                            symptom=re.sub(r'</a>','',symptom) # Remove hyperlink
                            symptom=re.sub(r'<[^<]+?>',' ',symptom) # All the tags
                            symptom=re.sub(r'\[.*\]','',symptom) # Remove citation text                     
                            symptom=symptom.replace("&gt",">")
                            ret+=data.get_text()+" - "+symptom+"\n"
#                            print(data.get_text(),"-",symptom)
                            filled = 1
                if filled:
                    break
    return ret

# returns the list of synonyms of the input word from thesaurus.com (https://www.thesaurus.com/) and wordnet (https://www.nltk.org/howto/wordnet.html)
def synonyms(term):
    synonyms = []
    response = requests.get('https://www.thesaurus.com/browse/{}'.format(term))
    soup = BeautifulSoup(response.content,  "html.parser")
    try:
        container=soup.find('section', {'class': 'MainContentContainer'}) 
        row=container.find('div',{'class':'css-191l5o0-ClassicContentCard'})
        row = row.find_all('li')
        for x in row:
            synonyms.append(x.get_text())
    except:
        None
    for syn in wordnet.synsets(term):
        synonyms+=syn.lemma_names()
    return set(synonyms)

# utlities for pre-processing
stop_words = stopwords.words('english')
lemmatizer = WordNetLemmatizer()
splitter = RegexpTokenizer(r'\w+')

"""**Disease Symptom dataset** was created in a separate python program.

**Dataset scrapping** was done using **NHP website** and **wikipedia data**

Disease Combination dataset contains the combinations for each of the disease present in dataset as practically it is often observed that it is not necessary for a person to have a disease when all the symptoms are faced by the patient or the user.

*To tackle this problem, combinations are made with the symptoms for each disease.*

 **This increases the size of the data exponentially and helps the model to predict the disease with much better accuracy.**

*df_comb -> Dataframe consisting of dataset generated by combining symptoms for each disease.*

*df_norm -> Dataframe consisting of dataset which contains a single row for each diseases with all the symptoms for that corresponding disease.*

**Dataset contains 261 diseases and their symptoms**
"""

# Load Dataset scraped from NHP (https://www.nhp.gov.in/disease-a-z) & Wikipedia
# Scrapping and creation of dataset csv is done in a separate program
df_comb = pd.read_csv("Health_Care_Unit/contents/dataset/dis_sym_dataset_comb.csv") # Disease combination
df_norm = pd.read_csv("Health_Care_Unit/contents/dataset/dis_sym_dataset_norm.csv") # Individual Disease

X = df_comb.iloc[:, 1:]
Y = df_comb.iloc[:, 0:1]

"""Using **Logistic Regression (LR) Classifier** as it gives better accuracy compared to other classification models as observed in the comparison of model accuracies in Model_latest.py

Cross validation is done on dataset with cv = 5
"""

# lr = LogisticRegression()
# lr = lr.fit(X, Y)
# # scores = cross_val_score(lr, X, Y, cv=5)

# pipeline_ls = make_pipeline(lemmatizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize,stop_words='english'), LogisticRegression())
# ##(r'\b(?:http|ftp)s?://\S*\w|\w+|[^\w\s]+') ([a-zA-Z]+)([0-9]+) -- these tolenizers giving me low accuray

# pickle.dump(lr,open('symptom.pkl','wb'))

X = df_norm.iloc[:, 1:]
Y = df_norm.iloc[:, 0:1]

# List of symptoms
dataset_symptoms = list(X.columns)

"""# Symptoms initially taken from user."""

def take_input(symptoms):
    # Taking symptoms from user as input 
    user_symptoms = symptoms.lower().split(',')
    # Preprocessing the input symptoms
    processed_user_symptoms=[]
    for sym in user_symptoms:
        sym=sym.strip()
        sym=sym.replace('-',' ')
        sym=sym.replace("'",'')
        sym = ' '.join([lemmatizer.lemmatize(word) for word in splitter.tokenize(sym)])
        processed_user_symptoms.append(sym)

    """Pre-processing on symptoms entered by user is done."""

    # Taking each user symptom and finding all its synonyms and appending it to the pre-processed symptom string
    user_symptoms = []
    for user_sym in processed_user_symptoms:
        user_sym = user_sym.split()
        str_sym = set()
        for comb in range(1, len(user_sym)+1):
            for subset in combinations(user_sym, comb):
                subset=' '.join(subset)
                subset = synonyms(subset) 
                str_sym.update(subset)
        str_sym.add(' '.join(user_sym))
        user_symptoms.append(' '.join(str_sym).replace('_',' '))
    # query expansion performed by joining synonyms found for each symptoms initially entered
    # print("After query expansion done by using the symptoms entered")
    # print(user_symptoms)

    """The below procedure is performed in order to show the symptom synonmys found for the symptoms entered by the user.

    The symptom synonyms and user symptoms are matched with the symptoms present in dataset. Only the symptoms which matches the symptoms present in dataset are shown back to the user. 
    """

    # Loop over all the symptoms in dataset and check its similarity score to the synonym string of the user-input 
    # symptoms. If similarity>0.5, add the symptom to the final list
    found_symptoms = set()
    for idx, data_sym in enumerate(dataset_symptoms):
        data_sym_split=data_sym.split()
        for user_sym in user_symptoms:
            count=0
            for symp in data_sym_split:
                if symp in user_sym.split():
                    count+=1
            if count/len(data_sym_split)>0.5:
                found_symptoms.add(data_sym)
    found_symptoms = list(found_symptoms)

    """## **Prompt the user to select the relevant symptoms by entering the corresponding indices.**"""

    # Print all found symptoms
    value = "Top matching symptoms from your search!\n"
    for idx, symp in enumerate(found_symptoms):
        value += str(idx)+":"+str(symp)+"\n"
    return value, found_symptoms

def co_occur(rel_symps, found_symptoms):
    
    # Show the related symptoms found in the dataset and ask user to select among them
    select_list = rel_symps.split()

    # Find other relevant symptoms from the dataset based on user symptoms based on the highest co-occurance with the
    # ones that is input by the user
    dis_list = set()
    final_symp = [] 
    counter_list = []
    for idx in select_list:
        symp=found_symptoms[int(idx)]
        final_symp.append(symp)
        dis_list.update(set(df_norm[df_norm[symp]==1]['label_dis']))
    
    for dis in dis_list:
        row = df_norm.loc[df_norm['label_dis'] == dis].values.tolist()
        row[0].pop(0)
        for idx,val in enumerate(row[0]):
            if val!=0 and dataset_symptoms[idx] not in final_symp:
                counter_list.append(dataset_symptoms[idx])

    """## To find symptoms which generally co-occur, for example with symptoms like cough, headache generally happens hence they co-occur."""

    # Symptoms that co-occur with the ones selected by user              
    dict_symp = dict(Counter(counter_list))
    dict_symp_tup = sorted(dict_symp.items(), key=operator.itemgetter(1),reverse=True)   
    #print(dict_symp_tup)

    """## User is presented with a list of co-occuring symptoms to select from and is performed iteratively to recommend more possible symptoms based on the similarity to the previously entered symptoms.

    As the co-occuring symptoms can be in overwhelming numbers, only the top 5 are recommended to the user from which user can select the symptoms.

    If user does not have any of those 5 symptoms and wants to see the next 5, he can do so by giving input as -1.

    To stop the recommendation, user needs to give input as "No".
    """

    found_symptoms=[]
    count=0
    value = "Common co-occuring symptoms:\n"
    for tup in dict_symp_tup:
        count+=1
        found_symptoms.append(tup[0])
        if count==len(dict_symp_tup):
            
            for idx,ele in enumerate(found_symptoms):
                value += str(idx)+":"+str(ele)+"\n"
            break

    return value, dict_symp_tup, final_symp

def final_pred(extra_symp, dict_symp_tup, final_symp):

    # Iteratively, suggest top co-occuring symptoms to the user and ask to select the ones applicable 
    found_symptoms=[]
    count=0
    for tup in dict_symp_tup:
        count+=1
        found_symptoms.append(tup[0])
        if count==len(dict_symp_tup):
            
            select_list = extra_symp.split()
            if select_list[0]=='no':
                break
            if select_list[0]=='-1':
                found_symptoms = [] 
                continue
            for idx in select_list:
                final_symp.append(found_symptoms[int(idx)])
            found_symptoms = []

    """Final Symptom list"""

    # Create query vector based on symptoms selected by the user
    # print("\nFinal list of Symptoms that will be used for prediction:")
    sample_x = [0 for x in range(0,len(dataset_symptoms))]
    for val in final_symp:
        # print(val)
        sample_x[dataset_symptoms.index(val)]=1

    """Prediction of disease is done"""

    # Predict disease
    # lr = LogisticRegression()
    # lr = lr.fit(X, Y)
    loaded_model = pickle.load(open('Health_Care_Unit/contents/symptom.pkl', 'rb'))
    prediction = loaded_model.predict_proba([sample_x])

    """Show top k diseases and their probabilities to the user.

    K in this case is 10
    """

    k = 10
    diseases = list(set(Y['label_dis']))
    diseases.sort()
    topk = prediction[0].argsort()[-k:][::-1]

    """# **Showing the list of top k diseases to the user with their prediction probabilities.**

    # **For getting information about the suggested treatments, user can enter the corresponding index to know more details.**
    """

    value = "Top 10 diseases predicted based on symptoms\n"
    topk_dict = {}
    # Show top 10 highly probable disease to the user.
    for idx,t in  enumerate(topk):
        match_sym=set()
        row = df_norm.loc[df_norm['label_dis'] == diseases[t]].values.tolist()
        row[0].pop(0)

        for idx,val in enumerate(row[0]):
            if val!=0:
                match_sym.add(dataset_symptoms[idx])
        prob = (len(match_sym.intersection(set(final_symp)))+1)/(len(set(final_symp))+1)
        # prob *= mean(scores)
        topk_dict[t] = prob
    j = 0
    topk_index_mapping = {}
    topk_sorted = dict(sorted(topk_dict.items(), key=lambda kv: kv[1], reverse=True))
    for key in topk_sorted:
        prob = topk_sorted[key]*100
        value += str(j) + " Disease name:"+str(diseases[key])+ "\tProbability:"+str(round(prob, 2))+"%"+"\n"
        topk_index_mapping[j] = key
        j += 1

    return value, topk_index_mapping, diseases

def more_dat(more, topk_index_mapping, diseases):

    
    if more!='-1':
        dis=diseases[topk_index_mapping[int(more)]]
        value = str(diseaseDetail(dis))
        return value

